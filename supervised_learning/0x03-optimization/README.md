# [holbertonschool-machine_learning](https://github.com/dalexach/holbertonschool-machine_learning)

# 0x03. Optimization
### Description 
This project is for learn about hyperparameter, normalize data, mini-batch,  sochastic gradient descent, moving average, Adam optimization, RMSProp, learning rate decay, batch normalization, moving average, gradient decent with momentum.

## Files
#### Mandatory Tasks

| File | Description |
| ------ | ------ |
| [0-norm_constants.py](0-norm_constants.py) | Function def normalization_constants that calculates the normalization (standardization) constants of a matrix. |
| [1-normalize.py](1-normalize.py) | Function normalize that normalizes (standardizes) a matrix. |
| [2-shuffle_data.py](2-shuffle_data.py) | Function shuffle_data that shuffles the data points in two matrices the same way. |
| [3-mini_batch.py](3-mini_batch.py) | Function train_mini_batch that trains a loaded neural network model using mini-batch gradient descent. |
| [4-moving_average.py](4-moving_average.py) | Function moving_average that calculates the weighted moving average of a data set. |
| [5-momentum.py](5-momentum.py) | Function update_variables_momentum that updates a variable using the gradient descent with momentum optimization algorithm. |
| [6-momentum.py](6-momentum.py) | Function create_momentum_op  that creates the training operation for a neural network in tensorflow using the gradient descent with momentum optimization algorithm. |
| [7-RMSProp.py](7-RMSProp.py) | Function update_variables_RMSProp that updates a variable using the RMSProp optimization algorithm. |
| [8-RMSProp.py](8-RMSProp.py) | Function create_RMSProp_op that creates the training operation for a neural network in tensorflow using the RMSProp optimization algorithm. |
| [9-Adam.py](9-Adam.py) | Function update_variables_Adam that updates a variable in place using the Adam optimization algorithm. |
| [10-Adam.py](10-Adam.py) | Function create_Adam_op that creates the training operation for a neural network in tensorflow using the Adam optimization algorithm. |
| [11-learning_rate_decay.py](11-learning_rate_decay.py) | Function learning_rate_decay that updates the learning rate using inverse time decay in numpy. |
| [12-learning_rate_decay.py](12-learning_rate_decay.py) | Function learning_rate_decay that creates a learning rate decay operation in tensorflow using inverse time decay. |
| [13-batch_norm.py](13-batch_norm.py) | Function batch_norm that normalizes an unactivated output of a neural network using batch normalization. |
| [14-batch_norm.py](14-batch_norm.py) | Function that creates a batch normalization layer for a neural network in tensorflow. |
| [15-model.py](15-model.py) | Function model that builds, trains, and saves a neural network model in tensorflow using Adam optimization, mini-batch gradient descent, learning rate decay, and batch normalization. |

## Build with
-Python (python 3.5)
-Numpy (numpy 1.15)
-Ubuntu 16.04 LTS 

## Author

[Daniela Chamorro](https://www.linkedin.com/in/dalexach/) [:octocat:](https://github.com/dalexach)

[Twitter](https://twitter.com/dalexach)
